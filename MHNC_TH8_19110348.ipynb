{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Giải thích vì sao khi dùng Naive-Learning thì xe taxi chỉ đứng yên một chỗ?**\n",
        "\n",
        "**Trả lời:** Naive-Learning ý tưởng chính là việc lựa chọn một action dẫn đến một kết quả  \"tốt\" thì “có nhiều khả năng” được chọn và nếu nó dẫn đến một kết quả “xấu” thì nó ít có khả năng được chọn hơn. Taxi vẫn có di chuyển khi vì quá trình khởi tạo có thể khiến xe rơi vào vị trí ngõ cụt, reward vẫn được nhận cho action này. Điều này bắt nguồn từ việc random các action ban đầu khiến xe thực hiện sai (đón và trả khách) nên các giá trị reward rất nhỏ dẫn đến ở các bước sau, agent sẽ không chọn các action này thay vào đó là các action di chuyển."
      ],
      "metadata": {
        "id": "HcJLfEFhyaRs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Giải thích vì sao khi dùng Q-Leaning thì xe taxi có thể đón và trả khách được?**\n",
        "\n",
        "**Trả lời:** Mục tiêu của Q-Learning là tìm ra các action tối ưu ở mỗi state. Q-Learning có:\n",
        "\n",
        "- Chọn hành động tốt nhất từ Q-table (exploitation)\n",
        "- Đôi khi gamble và chọn một action ngẫu nhiên (exploration)\n",
        "- Nếu giá trị hiển thị bằng 0 cho tất cả các tùy chọn, hãy chọn một hành động ngẫu nhiên.\n",
        "- Bắt đầu với 100% gamble (exploration),, di chuyển dần dần đến 0% (exploitation)\n",
        "- Sử dụng discount rate và learning rate\n",
        "\n",
        "Nên Q - Learning luôn có sự chênh lệch nhất định, ít trùng giá trị, dễ dàng tìm ra action tối ưu tại mỗi state."
      ],
      "metadata": {
        "id": "dAZTielfyaK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.Tìm hiểu một game khác trên OpenAI và thiết lập cho agent chơi được**"
      ],
      "metadata": {
        "id": "bLVCAnNL3ymN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code for training a Deep Reinforcement Learning agent to play the game of Snake**"
      ],
      "metadata": {
        "id": "mQTAxcGG4P3p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0p41oUhUWRt",
        "outputId": "f7296590-fd83-423a-8b55-487581093f9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'snake-rl'...\n",
            "remote: Enumerating objects: 615, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 615 (delta 29), reused 55 (delta 19), pack-reused 543\u001b[K\n",
            "Receiving objects: 100% (615/615), 61.10 MiB | 32.50 MiB/s, done.\n",
            "Resolving deltas: 100% (311/311), done.\n"
          ]
        }
      ],
      "source": [
        "# Clone  repo\n",
        "!git clone https://github.com/DragonWarrior15/snake-rl.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd '/content/snake-rl'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hImqZ63UbWp",
        "outputId": "e3743c64-ca00-4883-8dcd-78a0133a660e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/snake-rl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "store all the agents here\n",
        "\"\"\"\n",
        "from replay_buffer import ReplayBuffer, ReplayBufferNumpy\n",
        "import numpy as np\n",
        "import time\n",
        "import pickle\n",
        "from collections import deque\n",
        "import json\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Softmax, MaxPool2D\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "# from tensorflow.keras.losses import Huber\n",
        "\n",
        "def huber_loss(y_true, y_pred, delta=1):\n",
        "    \"\"\"Keras implementation for huber loss\n",
        "    loss = {\n",
        "        0.5 * (y_true - y_pred)**2 if abs(y_true - y_pred) < delta\n",
        "        delta * (abs(y_true - y_pred) - 0.5 * delta) otherwise\n",
        "    }\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : Tensor\n",
        "        The true values for the regression data\n",
        "    y_pred : Tensor\n",
        "        The predicted values for the regression data\n",
        "    delta : float, optional\n",
        "        The cutoff to decide whether to use quadratic or linear loss\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    loss : Tensor\n",
        "        loss values for all points\n",
        "    \"\"\"\n",
        "    error = (y_true - y_pred)\n",
        "    quad_error = 0.5*tf.math.square(error)\n",
        "    lin_error = delta*(tf.math.abs(error) - 0.5*delta)\n",
        "    # quadratic error, linear error\n",
        "    return tf.where(tf.math.abs(error) < delta, quad_error, lin_error)\n",
        "\n",
        "def mean_huber_loss(y_true, y_pred, delta=1):\n",
        "    \"\"\"Calculates the mean value of huber loss\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : Tensor\n",
        "        The true values for the regression data\n",
        "    y_pred : Tensor\n",
        "        The predicted values for the regression data\n",
        "    delta : float, optional\n",
        "        The cutoff to decide whether to use quadratic or linear loss\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    loss : Tensor\n",
        "        average loss across points\n",
        "    \"\"\"\n",
        "    return tf.reduce_mean(huber_loss(y_true, y_pred, delta))\n",
        "\n",
        "class Agent():\n",
        "    \"\"\"Base class for all agents\n",
        "    This class extends to the following classes\n",
        "    DeepQLearningAgent\n",
        "    HamiltonianCycleAgent\n",
        "    BreadthFirstSearchAgent\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    _board_size : int\n",
        "        Size of board, keep greater than 6 for useful learning\n",
        "        should be the same as the env board size\n",
        "    _n_frames : int\n",
        "        Total frames to keep in history when making prediction\n",
        "        should be the same as env board size\n",
        "    _buffer_size : int\n",
        "        Size of the buffer, how many examples to keep in memory\n",
        "        should be large for DQN\n",
        "    _n_actions : int\n",
        "        Total actions available in the env, should be same as env\n",
        "    _gamma : float\n",
        "        Reward discounting to use for future rewards, useful in policy\n",
        "        gradient, keep < 1 for convergence\n",
        "    _use_target_net : bool\n",
        "        If use a target network to calculate next state Q values,\n",
        "        necessary to stabilise DQN learning\n",
        "    _input_shape : tuple\n",
        "        Tuple to store individual state shapes\n",
        "    _board_grid : Numpy array\n",
        "        A square filled with values from 0 to board size **2,\n",
        "        Useful when converting between row, col and int representation\n",
        "    _version : str\n",
        "        model version string\n",
        "    \"\"\"\n",
        "    def __init__(self, board_size=10, frames=2, buffer_size=10000,\n",
        "                 gamma=0.99, n_actions=3, use_target_net=True,\n",
        "                 version=''):\n",
        "        \"\"\" initialize the agent\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        board_size : int, optional\n",
        "            The env board size, keep > 6\n",
        "        frames : int, optional\n",
        "            The env frame count to keep old frames in state\n",
        "        buffer_size : int, optional\n",
        "            Size of the buffer, keep large for DQN\n",
        "        gamma : float, optional\n",
        "            Agent's discount factor, keep < 1 for convergence\n",
        "        n_actions : int, optional\n",
        "            Count of actions available in env\n",
        "        use_target_net : bool, optional\n",
        "            Whether to use target network, necessary for DQN convergence\n",
        "        version : str, optional except NN based models\n",
        "            path to the model architecture json\n",
        "        \"\"\"\n",
        "        self._board_size = board_size\n",
        "        self._n_frames = frames\n",
        "        self._buffer_size = buffer_size\n",
        "        self._n_actions = n_actions\n",
        "        self._gamma = gamma\n",
        "        self._use_target_net = use_target_net\n",
        "        self._input_shape = (self._board_size, self._board_size, self._n_frames)\n",
        "        # reset buffer also initializes the buffer\n",
        "        self.reset_buffer()\n",
        "        self._board_grid = np.arange(0, self._board_size**2)\\\n",
        "                             .reshape(self._board_size, -1)\n",
        "        self._version = version\n",
        "\n",
        "    def get_gamma(self):\n",
        "        \"\"\"Returns the agent's gamma value\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        _gamma : float\n",
        "            Agent's gamma value\n",
        "        \"\"\"\n",
        "        return self._gamma\n",
        "\n",
        "    def reset_buffer(self, buffer_size=None):\n",
        "        \"\"\"Reset current buffer \n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        buffer_size : int, optional\n",
        "            Initialize the buffer with buffer_size, if not supplied,\n",
        "            use the original value\n",
        "        \"\"\"\n",
        "        if(buffer_size is not None):\n",
        "            self._buffer_size = buffer_size\n",
        "        self._buffer = ReplayBufferNumpy(self._buffer_size, self._board_size, \n",
        "                                    self._n_frames, self._n_actions)\n",
        "\n",
        "    def get_buffer_size(self):\n",
        "        \"\"\"Get the current buffer size\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        buffer size : int\n",
        "            Current size of the buffer\n",
        "        \"\"\"\n",
        "        return self._buffer.get_current_size()\n",
        "\n",
        "    def add_to_buffer(self, board, action, reward, next_board, done, legal_moves):\n",
        "        \"\"\"Add current game step to the replay buffer\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        board : Numpy array\n",
        "            Current state of the board, can contain multiple games\n",
        "        action : Numpy array or int\n",
        "            Action that was taken, can contain actions for multiple games\n",
        "        reward : Numpy array or int\n",
        "            Reward value(s) for the current action on current states\n",
        "        next_board : Numpy array\n",
        "            State obtained after executing action on current state\n",
        "        done : Numpy array or int\n",
        "            Binary indicator for game termination\n",
        "        legal_moves : Numpy array\n",
        "            Binary indicators for actions which are allowed at next states\n",
        "        \"\"\"\n",
        "        self._buffer.add_to_buffer(board, action, reward, next_board, \n",
        "                                   done, legal_moves)\n",
        "\n",
        "    def save_buffer(self, file_path='', iteration=None):\n",
        "        \"\"\"Save the buffer to disk\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        file_path : str, optional\n",
        "            The location to save the buffer at\n",
        "        iteration : int, optional\n",
        "            Iteration number to tag the file name with, if None, iteration is 0\n",
        "        \"\"\"\n",
        "        if(iteration is not None):\n",
        "            assert isinstance(iteration, int), \"iteration should be an integer\"\n",
        "        else:\n",
        "            iteration = 0\n",
        "        with open(\"{}/buffer_{:04d}\".format(file_path, iteration), 'wb') as f:\n",
        "            pickle.dump(self._buffer, f)\n",
        "\n",
        "    def load_buffer(self, file_path='', iteration=None):\n",
        "        \"\"\"Load the buffer from disk\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        file_path : str, optional\n",
        "            Disk location to fetch the buffer from\n",
        "        iteration : int, optional\n",
        "            Iteration number to use in case the file has been tagged\n",
        "            with one, 0 if iteration is None\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        FileNotFoundError\n",
        "            If the requested file could not be located on the disk\n",
        "        \"\"\"\n",
        "        if(iteration is not None):\n",
        "            assert isinstance(iteration, int), \"iteration should be an integer\"\n",
        "        else:\n",
        "            iteration = 0\n",
        "        with open(\"{}/buffer_{:04d}\".format(file_path, iteration), 'rb') as f:\n",
        "            self._buffer = pickle.load(f)\n",
        "\n",
        "    def _point_to_row_col(self, point):\n",
        "        \"\"\"Covert a point value to row, col value\n",
        "        point value is the array index when it is flattened\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        point : int\n",
        "            The point to convert\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        (row, col) : tuple\n",
        "            Row and column values for the point\n",
        "        \"\"\"\n",
        "        return (point//self._board_size, point%self._board_size)\n",
        "\n",
        "    def _row_col_to_point(self, row, col):\n",
        "        \"\"\"Covert a (row, col) to value\n",
        "        point value is the array index when it is flattened\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        row : int\n",
        "            The row number in array\n",
        "        col : int\n",
        "            The column number in array\n",
        "        Returns\n",
        "        -------\n",
        "        point : int\n",
        "            point value corresponding to the row and col values\n",
        "        \"\"\"\n",
        "        return row*self._board_size + col\n",
        "\n",
        "class DeepQLearningAgent(Agent):\n",
        "    \"\"\"This agent learns the game via Q learning\n",
        "    model outputs everywhere refers to Q values\n",
        "    This class extends to the following classes\n",
        "    PolicyGradientAgent\n",
        "    AdvantageActorCriticAgent\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    _model : TensorFlow Graph\n",
        "        Stores the graph of the DQN model\n",
        "    _target_net : TensorFlow Graph\n",
        "        Stores the target network graph of the DQN model\n",
        "    \"\"\"\n",
        "    def __init__(self, board_size=10, frames=4, buffer_size=10000,\n",
        "                 gamma=0.99, n_actions=3, use_target_net=True,\n",
        "                 version=''):\n",
        "        \"\"\"Initializer for DQN agent, arguments are same as Agent class\n",
        "        except use_target_net is by default True and we call and additional\n",
        "        reset models method to initialize the DQN networks\n",
        "        \"\"\"\n",
        "        Agent.__init__(self, board_size=board_size, frames=frames, buffer_size=buffer_size,\n",
        "                 gamma=gamma, n_actions=n_actions, use_target_net=use_target_net,\n",
        "                 version=version)\n",
        "        self.reset_models()\n",
        "\n",
        "    def reset_models(self):\n",
        "        \"\"\" Reset all the models by creating new graphs\"\"\"\n",
        "        self._model = self._agent_model()\n",
        "        if(self._use_target_net):\n",
        "            self._target_net = self._agent_model()\n",
        "            self.update_target_net()\n",
        "\n",
        "    def _prepare_input(self, board):\n",
        "        \"\"\"Reshape input and normalize\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        board : Numpy array\n",
        "            The board state to process\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        board : Numpy array\n",
        "            Processed and normalized board\n",
        "        \"\"\"\n",
        "        if(board.ndim == 3):\n",
        "            board = board.reshape((1,) + self._input_shape)\n",
        "        board = self._normalize_board(board.copy())\n",
        "        return board.copy()\n",
        "\n",
        "    def _get_model_outputs(self, board, model=None):\n",
        "        \"\"\"Get action values from the DQN model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        board : Numpy array\n",
        "            The board state for which to predict action values\n",
        "        model : TensorFlow Graph, optional\n",
        "            The graph to use for prediction, model or target network\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        model_outputs : Numpy array\n",
        "            Predicted model outputs on board, \n",
        "            of shape board.shape[0] * num actions\n",
        "        \"\"\"\n",
        "        # to correct dimensions and normalize\n",
        "        board = self._prepare_input(board)\n",
        "        # the default model to use\n",
        "        if model is None:\n",
        "            model = self._model\n",
        "        model_outputs = model.predict_on_batch(board)\n",
        "        return model_outputs\n",
        "\n",
        "    def _normalize_board(self, board):\n",
        "        \"\"\"Normalize the board before input to the network\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        board : Numpy array\n",
        "            The board state to normalize\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        board : Numpy array\n",
        "            The copy of board state after normalization\n",
        "        \"\"\"\n",
        "        # return board.copy()\n",
        "        # return((board/128.0 - 1).copy())\n",
        "        return board.astype(np.float32)/4.0\n",
        "\n",
        "    def move(self, board, legal_moves, value=None):\n",
        "        \"\"\"Get the action with maximum Q value\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        board : Numpy array\n",
        "            The board state on which to calculate best action\n",
        "        value : None, optional\n",
        "            Kept for consistency with other agent classes\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        output : Numpy array\n",
        "            Selected action using the argmax function\n",
        "        \"\"\"\n",
        "        # use the agent model to make the predictions\n",
        "        model_outputs = self._get_model_outputs(board, self._model)\n",
        "        return np.argmax(np.where(legal_moves==1, model_outputs, -np.inf), axis=1)\n",
        "\n",
        "    def _agent_model(self):\n",
        "        \"\"\"Returns the model which evaluates Q values for a given state input\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        model : TensorFlow Graph\n",
        "            DQN model graph\n",
        "        \"\"\"\n",
        "        # define the input layer, shape is dependent on the board size and frames\n",
        "        with open('model_config/{:s}.json'.format(self._version), 'r') as f:\n",
        "            m = json.loads(f.read())\n",
        "        \n",
        "        input_board = Input((self._board_size, self._board_size, self._n_frames,), name='input')\n",
        "        x = input_board\n",
        "        for layer in m['model']:\n",
        "            l = m['model'][layer]\n",
        "            if('Conv2D' in layer):\n",
        "                # add convolutional layer\n",
        "                x = Conv2D(**l)(x)\n",
        "            if('Flatten' in layer):\n",
        "                x = Flatten()(x)\n",
        "            if('Dense' in layer):\n",
        "                x = Dense(**l)(x)\n",
        "        out = Dense(self._n_actions, activation='linear', name='action_values')(x)\n",
        "        model = Model(inputs=input_board, outputs=out)\n",
        "        model.compile(optimizer=RMSprop(0.0005), loss=mean_huber_loss)\n",
        "                \n",
        "        \"\"\"\n",
        "        input_board = Input((self._board_size, self._board_size, self._n_frames,), name='input')\n",
        "        x = Conv2D(16, (3,3), activation='relu', data_format='channels_last')(input_board)\n",
        "        x = Conv2D(32, (3,3), activation='relu', data_format='channels_last')(x)\n",
        "        x = Conv2D(64, (6,6), activation='relu', data_format='channels_last')(x)\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(64, activation = 'relu', name='action_prev_dense')(x)\n",
        "        # this layer contains the final output values, activation is linear since\n",
        "        # the loss used is huber or mse\n",
        "        out = Dense(self._n_actions, activation='linear', name='action_values')(x)\n",
        "        # compile the model\n",
        "        model = Model(inputs=input_board, outputs=out)\n",
        "        model.compile(optimizer=RMSprop(0.0005), loss=mean_huber_loss)\n",
        "        # model.compile(optimizer=RMSprop(0.0005), loss='mean_squared_error')\n",
        "        \"\"\"\n",
        "\n",
        "        return model\n",
        "\n",
        "    def set_weights_trainable(self):\n",
        "        \"\"\"Set selected layers to non trainable and compile the model\"\"\"\n",
        "        for layer in self._model.layers:\n",
        "            layer.trainable = False\n",
        "        # the last dense layers should be trainable\n",
        "        for s in ['action_prev_dense', 'action_values']:\n",
        "            self._model.get_layer(s).trainable = True\n",
        "        self._model.compile(optimizer = self._model.optimizer, \n",
        "                            loss = self._model.loss)\n",
        "\n",
        "\n",
        "    def get_action_proba(self, board, values=None):\n",
        "        \"\"\"Returns the action probability values using the DQN model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        board : Numpy array\n",
        "            Board state on which to calculate action probabilities\n",
        "        values : None, optional\n",
        "            Kept for consistency with other agent classes\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        model_outputs : Numpy array\n",
        "            Action probabilities, shape is board.shape[0] * n_actions\n",
        "        \"\"\"\n",
        "        model_outputs = self._get_model_outputs(board, self._model)\n",
        "        # subtracting max and taking softmax does not change output\n",
        "        # do this for numerical stability\n",
        "        model_outputs = np.clip(model_outputs, -10, 10)\n",
        "        model_outputs = model_outputs - model_outputs.max(axis=1).reshape((-1,1))\n",
        "        model_outputs = np.exp(model_outputs)\n",
        "        model_outputs = model_outputs/model_outputs.sum(axis=1).reshape((-1,1))\n",
        "        return model_outputs\n",
        "\n",
        "    def save_model(self, file_path='', iteration=None):\n",
        "        \"\"\"Save the current models to disk using tensorflow's\n",
        "        inbuilt save model function (saves in h5 format)\n",
        "        saving weights instead of model as cannot load compiled\n",
        "        model with any kind of custom object (loss or metric)\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        file_path : str, optional\n",
        "            Path where to save the file\n",
        "        iteration : int, optional\n",
        "            Iteration number to tag the file name with, if None, iteration is 0\n",
        "        \"\"\"\n",
        "        if(iteration is not None):\n",
        "            assert isinstance(iteration, int), \"iteration should be an integer\"\n",
        "        else:\n",
        "            iteration = 0\n",
        "        self._model.save_weights(\"{}/model_{:04d}.h5\".format(file_path, iteration))\n",
        "        if(self._use_target_net):\n",
        "            self._target_net.save_weights(\"{}/model_{:04d}_target.h5\".format(file_path, iteration))\n",
        "\n",
        "    def load_model(self, file_path='', iteration=None):\n",
        "        \"\"\" load any existing models, if available \"\"\"\n",
        "        \"\"\"Load models from disk using tensorflow's\n",
        "        inbuilt load model function (model saved in h5 format)\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        file_path : str, optional\n",
        "            Path where to find the file\n",
        "        iteration : int, optional\n",
        "            Iteration number the file is tagged with, if None, iteration is 0\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        FileNotFoundError\n",
        "            The file is not loaded if not found and an error message is printed,\n",
        "            this error does not affect the functioning of the program\n",
        "        \"\"\"\n",
        "        if(iteration is not None):\n",
        "            assert isinstance(iteration, int), \"iteration should be an integer\"\n",
        "        else:\n",
        "            iteration = 0\n",
        "        self._model.load_weights(\"{}/model_{:04d}.h5\".format(file_path, iteration))\n",
        "        if(self._use_target_net):\n",
        "            self._target_net.load_weights(\"{}/model_{:04d}_target.h5\".format(file_path, iteration))\n",
        "        # print(\"Couldn't locate models at {}, check provided path\".format(file_path))\n",
        "\n",
        "    def print_models(self):\n",
        "        \"\"\"Print the current models using summary method\"\"\"\n",
        "        print('Training Model')\n",
        "        print(self._model.summary())\n",
        "        if(self._use_target_net):\n",
        "            print('Target Network')\n",
        "            print(self._target_net.summary())\n",
        "\n",
        "    def train_agent(self, batch_size=32, num_games=1, reward_clip=False):\n",
        "        \"\"\"Train the model by sampling from buffer and return the error.\n",
        "        We are predicting the expected future discounted reward for all\n",
        "        actions with our model. The target for training the model is calculated\n",
        "        in two parts:\n",
        "        1) dicounted reward = current reward + \n",
        "                        (max possible reward in next state) * gamma\n",
        "           the next reward component is calculated using the predictions\n",
        "           of the target network (for stability)\n",
        "        2) rewards for only the action take are compared, hence while\n",
        "           calculating the target, set target value for all other actions\n",
        "           the same as the model predictions\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size : int, optional\n",
        "            The number of examples to sample from buffer, should be small\n",
        "        num_games : int, optional\n",
        "            Not used here, kept for consistency with other agents\n",
        "        reward_clip : bool, optional\n",
        "            Whether to clip the rewards using the numpy sign command\n",
        "            rewards > 0 -> 1, rewards <0 -> -1, rewards == 0 remain same\n",
        "            this setting can alter the learned behaviour of the agent\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            loss : float\n",
        "            The current error (error metric is defined in reset_models)\n",
        "        \"\"\"\n",
        "        s, a, r, next_s, done, legal_moves = self._buffer.sample(batch_size)\n",
        "        if(reward_clip):\n",
        "            r = np.sign(r)\n",
        "        # calculate the discounted reward, and then train accordingly\n",
        "        current_model = self._target_net if self._use_target_net else self._model\n",
        "        next_model_outputs = self._get_model_outputs(next_s, current_model)\n",
        "        # our estimate of expexted future discounted reward\n",
        "        discounted_reward = r + \\\n",
        "            (self._gamma * np.max(np.where(legal_moves==1, next_model_outputs, -np.inf), \n",
        "                                  axis = 1)\\\n",
        "                                  .reshape(-1, 1)) * (1-done)\n",
        "        # create the target variable, only the column with action has different value\n",
        "        target = self._get_model_outputs(s)\n",
        "        # we bother only with the difference in reward estimate at the selected action\n",
        "        target = (1-a)*target + a*discounted_reward\n",
        "        # fit\n",
        "        loss = self._model.train_on_batch(self._normalize_board(s), target)\n",
        "        # loss = round(loss, 5)\n",
        "        return loss\n",
        "\n",
        "    def update_target_net(self):\n",
        "        \"\"\"Update the weights of the target network, which is kept\n",
        "        static for a few iterations to stabilize the other network.\n",
        "        This should not be updated very frequently\n",
        "        \"\"\"\n",
        "        if(self._use_target_net):\n",
        "            self._target_net.set_weights(self._model.get_weights())\n",
        "\n",
        "    def compare_weights(self):\n",
        "        \"\"\"Simple utility function to heck if the model and target \n",
        "        network have the same weights or not\n",
        "        \"\"\"\n",
        "        for i in range(len(self._model.layers)):\n",
        "            for j in range(len(self._model.layers[i].weights)):\n",
        "                c = (self._model.layers[i].weights[j].numpy() == \\\n",
        "                     self._target_net.layers[i].weights[j].numpy()).all()\n",
        "                print('Layer {:d} Weights {:d} Match : {:d}'.format(i, j, int(c)))\n",
        "\n",
        "    def copy_weights_from_agent(self, agent_for_copy):\n",
        "        \"\"\"Update weights between competing agents which can be used\n",
        "        in parallel training\n",
        "        \"\"\"\n",
        "        assert isinstance(agent_for_copy, self), \"Agent type is required for copy\"\n",
        "\n",
        "        self._model.set_weights(agent_for_copy._model.get_weights())\n",
        "        self._target_net.set_weights(agent_for_copy._model_pred.get_weights())\n",
        "\n",
        "class PolicyGradientAgent(DeepQLearningAgent):\n",
        "    \"\"\"This agent learns via Policy Gradient method\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    _update_function : function\n",
        "        defines the policy update function to use while training\n",
        "    \"\"\"\n",
        "    def __init__(self, board_size=10, frames=4, buffer_size=10000,\n",
        "                 gamma = 0.99, n_actions=3, use_target_net=False,\n",
        "                 version=''):\n",
        "        \"\"\"Initializer for PolicyGradientAgent, similar to DeepQLearningAgent\n",
        "        but does an extra assignment to the training function\n",
        "        \"\"\"\n",
        "        DeepQLearningAgent.__init__(self, board_size=board_size, frames=frames,\n",
        "                                buffer_size=buffer_size, gamma=gamma,\n",
        "                                n_actions=n_actions, use_target_net=False,\n",
        "                                version=version)\n",
        "        self._actor_optimizer = tf.keras.optimizer.Adam(1e-6)\n",
        "\n",
        "    def _agent_model(self):\n",
        "        \"\"\"Returns the model which evaluates prob values for a given state input\n",
        "        Model is compiled in a different function\n",
        "        Overrides parent\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        model : TensorFlow Graph\n",
        "            Policy Gradient model graph\n",
        "        \"\"\"\n",
        "        input_board = Input((self._board_size, self._board_size, self._n_frames,))\n",
        "        x = Conv2D(16, (4,4), activation = 'relu', data_format='channels_last', kernel_regularizer=l2(0.01))(input_board)\n",
        "        x = Conv2D(32, (4,4), activation = 'relu', data_format='channels_last', kernel_regularizer=l2(0.01))(x)\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(64, activation = 'relu', kernel_regularizer=l2(0.01))(x)\n",
        "        out = Dense(self._n_actions, activation = 'linear', name = 'action_logits', kernel_regularizer=l2(0.01))(x)\n",
        "\n",
        "        model = Model(inputs = input_board, outputs = out)\n",
        "        # do not compile the model here, but rather use the outputs separately\n",
        "        # in a training function to create any custom loss function\n",
        "        # model.compile(optimizer = RMSprop(0.0005), loss = 'mean_squared_error')\n",
        "        return model\n",
        "\n",
        "    def train_agent(self, batch_size=32, beta=0.1, normalize_rewards=False,\n",
        "                    num_games=1, reward_clip=False):\n",
        "        \"\"\"Train the model by sampling from buffer and return the error\n",
        "        The buffer is assumed to contain all states of a finite set of games\n",
        "        and is fully sampled from the buffer\n",
        "        Overrides parent\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size : int, optional\n",
        "            Not used here, kept for consistency with other agents\n",
        "        beta : float, optional\n",
        "            The weight for the entropy loss\n",
        "        normalize_rewards : bool, optional\n",
        "            Whether to normalize rewards for stable training\n",
        "        num_games : int, optional\n",
        "            Total games played in the current batch\n",
        "        reward_clip : bool, optional\n",
        "            Not used here, kept for consistency with other agents\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        error : list\n",
        "            The current loss (total loss, classification loss, entropy)\n",
        "        \"\"\"\n",
        "        # in policy gradient, only complete episodes are used for training\n",
        "        s, a, r, _, _, _ = self._buffer.sample(self._buffer.get_current_size())\n",
        "        # unlike DQN, the discounted reward is not estimated but true one\n",
        "        # we have defined custom policy graident loss function above\n",
        "        # use that to train to agent model\n",
        "        # normzlize the rewards for training stability\n",
        "        if(normalize_rewards):\n",
        "            r = (r - np.mean(r))/(np.std(r) + 1e-8)\n",
        "        target = np.multiply(a, r)\n",
        "        loss = actor_loss_update(self._prepare_input(s), target, self._model,\n",
        "                  self._actor_optimizer, beta=beta, num_games=num_games)\n",
        "        return loss[0] if len(loss)==1 else loss\n",
        "\n",
        "class AdvantageActorCriticAgent(PolicyGradientAgent):\n",
        "    \"\"\"This agent uses the Advantage Actor Critic method to train\n",
        "    the reinforcement learning agent, we will use Q actor critic here\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    _action_values_model : Tensorflow Graph\n",
        "        Contains the network for the action values calculation model\n",
        "    _actor_update : function\n",
        "        Custom function to prepare the \n",
        "    \"\"\"\n",
        "    def __init__(self, board_size=10, frames=4, buffer_size=10000,\n",
        "                 gamma = 0.99, n_actions=3, use_target_net=True,\n",
        "                 version=''):\n",
        "        DeepQLearningAgent.__init__(self, board_size=board_size, frames=frames,\n",
        "                                buffer_size=buffer_size, gamma=gamma,\n",
        "                                n_actions=n_actions, use_target_net=use_target_net,\n",
        "                                version=version)\n",
        "        self._optimizer = tf.keras.optimizers.RMSprop(5e-4)\n",
        "\n",
        "    def _agent_model(self):\n",
        "        \"\"\"Returns the models which evaluate prob logits and action values \n",
        "        for a given state input, Model is compiled in a different function\n",
        "        Overrides parent\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        model_logits : TensorFlow Graph\n",
        "            A2C model graph for action logits\n",
        "        model_full : TensorFlow Graph\n",
        "            A2C model complete graph\n",
        "        \"\"\"\n",
        "        input_board = Input((self._board_size, self._board_size, self._n_frames,))\n",
        "        x = Conv2D(16, (3,3), activation='relu', data_format='channels_last')(input_board)\n",
        "        x = Conv2D(32, (3,3), activation='relu', data_format='channels_last')(x)\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(64, activation='relu', name='dense')(x)\n",
        "        action_logits = Dense(self._n_actions, activation='linear', name='action_logits')(x)\n",
        "        state_values = Dense(1, activation='linear', name='state_values')(x)\n",
        "\n",
        "        model_logits = Model(inputs=input_board, outputs=action_logits)\n",
        "        model_full = Model(inputs=input_board, outputs=[action_logits, state_values])\n",
        "        model_values = Model(inputs=input_board, outputs=state_values)\n",
        "        # updates are calculated in the train_agent function\n",
        "\n",
        "        return model_logits, model_full, model_values\n",
        "\n",
        "    def reset_models(self):\n",
        "        \"\"\" Reset all the models by creating new graphs\"\"\"\n",
        "        self._model, self._full_model, self._values_model = self._agent_model()\n",
        "        if(self._use_target_net):\n",
        "            _, _, self._target_net = self._agent_model()\n",
        "            self.update_target_net()\n",
        "\n",
        "    def save_model(self, file_path='', iteration=None):\n",
        "        \"\"\"Save the current models to disk using tensorflow's\n",
        "        inbuilt save model function (saves in h5 format)\n",
        "        saving weights instead of model as cannot load compiled\n",
        "        model with any kind of custom object (loss or metric)\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        file_path : str, optional\n",
        "            Path where to save the file\n",
        "        iteration : int, optional\n",
        "            Iteration number to tag the file name with, if None, iteration is 0\n",
        "        \"\"\"\n",
        "        if(iteration is not None):\n",
        "            assert isinstance(iteration, int), \"iteration should be an integer\"\n",
        "        else:\n",
        "            iteration = 0\n",
        "        self._model.save_weights(\"{}/model_{:04d}.h5\".format(file_path, iteration))\n",
        "        self._full_model.save_weights(\"{}/model_{:04d}_full.h5\".format(file_path, iteration))\n",
        "        if(self._use_target_net):\n",
        "            self._values_model.save_weights(\"{}/model_{:04d}_values.h5\".format(file_path, iteration))\n",
        "            self._target_net.save_weights(\"{}/model_{:04d}_target.h5\".format(file_path, iteration))\n",
        "\n",
        "    def load_model(self, file_path='', iteration=None):\n",
        "        \"\"\" load any existing models, if available \"\"\"\n",
        "        \"\"\"Load models from disk using tensorflow's\n",
        "        inbuilt load model function (model saved in h5 format)\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        file_path : str, optional\n",
        "            Path where to find the file\n",
        "        iteration : int, optional\n",
        "            Iteration number the file is tagged with, if None, iteration is 0\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        FileNotFoundError\n",
        "            The file is not loaded if not found and an error message is printed,\n",
        "            this error does not affect the functioning of the program\n",
        "        \"\"\"\n",
        "        if(iteration is not None):\n",
        "            assert isinstance(iteration, int), \"iteration should be an integer\"\n",
        "        else:\n",
        "            iteration = 0\n",
        "        self._model.load_weights(\"{}/model_{:04d}.h5\".format(file_path, iteration))\n",
        "        self._full_model.load_weights(\"{}/model_{:04d}_full.h5\".format(file_path, iteration))\n",
        "        if(self._use_target_net):\n",
        "            self._values_model.load_weights(\"{}/model_{:04d}_values.h5\".format(file_path, iteration))\n",
        "            self._target_net.load_weights(\"{}/model_{:04d}_target.h5\".format(file_path, iteration))\n",
        "\n",
        "    def update_target_net(self):\n",
        "        \"\"\"Update the weights of the target network, which is kept\n",
        "        static for a few iterations to stabilize the other network.\n",
        "        This should not be updated very frequently\n",
        "        \"\"\"\n",
        "        if(self._use_target_net):\n",
        "            self._target_net.set_weights(self._values_model.get_weights())\n",
        "\n",
        "    def train_agent(self, batch_size=32, beta=0.001, normalize_rewards=False,\n",
        "                    num_games=1, reward_clip=False):\n",
        "        \"\"\"Train the model by sampling from buffer and return the error\n",
        "        The buffer is assumed to contain all states of a finite set of games\n",
        "        and is fully sampled from the buffer\n",
        "        Overrides parent\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size : int, optional\n",
        "            Not used here, kept for consistency with other agents\n",
        "        beta : float, optional\n",
        "            The weight for the policy gradient entropy loss\n",
        "        normalize_rewards : bool, optional\n",
        "            Whether to normalize rewards for stable training\n",
        "        num_games : int, optional\n",
        "            Not used here, kept for consistency with other agents\n",
        "        reward_clip : bool, optional\n",
        "            Not used here, kept for consistency with other agents\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        error : list\n",
        "            The current loss (total loss, actor loss, critic loss)\n",
        "        \"\"\"\n",
        "        # in policy gradient, only one complete episode is used for training\n",
        "        s, a, r, next_s, done, _ = self._buffer.sample(self._buffer.get_current_size())\n",
        "        s_prepared = self._prepare_input(s)\n",
        "        next_s_prepared = self._prepare_input(next_s)\n",
        "        # unlike DQN, the discounted reward is not estimated\n",
        "        # we have defined custom actor and critic losses functions above\n",
        "        # use that to train to agent model\n",
        "\n",
        "        # normzlize the rewards for training stability, does not work in practice\n",
        "        if(normalize_rewards):\n",
        "            if((r == r[0][0]).sum() == r.shape[0]):\n",
        "                # std dev is zero\n",
        "                r -= r\n",
        "            else:\n",
        "                r = (r - np.mean(r))/np.std(r)\n",
        "\n",
        "        if(reward_clip):\n",
        "            r = np.sign(r)\n",
        "\n",
        "        # calculate V values\n",
        "        if(self._use_target_net):\n",
        "            next_s_pred = self._target_net.predict_on_batch(next_s_prepared)\n",
        "        else:\n",
        "            next_s_pred = self._values_model.predict_on_batch(next_s_prepared)\n",
        "        s_pred = self._values_model.predict_on_batch(s_prepared)\n",
        "        \n",
        "        # prepare target\n",
        "        future_reward = self._gamma * next_s_pred * (1-done)\n",
        "        # calculate target for actor (uses advantage), similar to Policy Gradient\n",
        "        advantage = a * (r + future_reward - s_pred)\n",
        "\n",
        "        # calculate target for critic, simply current reward + future expected reward\n",
        "        critic_target = r + future_reward\n",
        "\n",
        "        model = self._full_model\n",
        "        with tf.GradientTape() as tape:\n",
        "            model_out = model(s_prepared)\n",
        "            policy = tf.nn.softmax(model_out[0])\n",
        "            log_policy = tf.nn.log_softmax(model_out[0])\n",
        "            # calculate loss\n",
        "            J = tf.reduce_sum(tf.multiply(advantage, log_policy))/num_games\n",
        "            entropy = -tf.reduce_sum(tf.multiply(policy, log_policy))/num_games\n",
        "            actor_loss = -J - beta*entropy\n",
        "            critic_loss = mean_huber_loss(critic_target, model_out[1])\n",
        "            loss = actor_loss + critic_loss\n",
        "        # get the gradients\n",
        "        grads = tape.gradient(loss, model.trainable_weights)\n",
        "        # grads = [tf.clip_by_value(grad, -5, 5) for grad in grads]\n",
        "        # run the optimizer\n",
        "        self._optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        loss = [loss.numpy(), actor_loss.numpy(), critic_loss.numpy()]\n",
        "        return loss[0] if len(loss)==1 else loss\n",
        "\n",
        "class HamiltonianCycleAgent(Agent):\n",
        "    \"\"\"This agent prepares a Hamiltonian Cycle through the board and then\n",
        "    follows it to reach the food, inherits Agent\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "        board_size (int): side length of the board\n",
        "        frames (int): no of frames available in one board state\n",
        "        n_actions (int): no of actions available in the action space\n",
        "    \"\"\"\n",
        "    def __init__(self, board_size=10, frames=4, buffer_size=10000,\n",
        "                 gamma = 0.99, n_actions=3, use_target_net=False,\n",
        "                 version=''):\n",
        "        assert board_size%2 == 0, \"Board size should be odd for hamiltonian cycle\"\n",
        "        Agent.__init__(self, board_size=board_size, frames=frames, buffer_size=buffer_size,\n",
        "                 gamma=gamma, n_actions=n_actions, use_target_net=use_target_net,\n",
        "                 version=version)\n",
        "        # self._get_cycle()\n",
        "        self._get_cycle_square()\n",
        "    \n",
        "    def _get_neighbors(self, point):\n",
        "        \"\"\"\n",
        "        point is a single integer such that \n",
        "        row = point//self._board_size\n",
        "        col = point%self._board_size\n",
        "        \"\"\"\n",
        "        row, col = point//self._board_size, point%self._board_size\n",
        "        neighbors = []\n",
        "        for delta_row, delta_col in [[-1,0], [1,0], [0,1], [0,-1]]:\n",
        "            new_row, new_col = row + delta_row, col + delta_col\n",
        "            if(1 <= new_row and new_row <= self._board_size-2 and\\\n",
        "               1 <= new_col and new_col <= self._board_size-2):\n",
        "                neighbors.append(new_row*self._board_size + new_col)\n",
        "        return neighbors\n",
        "\n",
        "    def _hamil_util(self):\n",
        "        neighbors = self._get_neighbors(self._cycle[self._index])\n",
        "        if(self._index == ((self._board_size-2)**2)-1):\n",
        "            if(self._start_point in neighbors):\n",
        "                # end of path and cycle\n",
        "                return True\n",
        "            else:\n",
        "                # end of path but not cycle\n",
        "                return False\n",
        "        else:\n",
        "            for i in neighbors:\n",
        "                if(i not in self._cycle_set):\n",
        "                    self._index += 1\n",
        "                    self._cycle[self._index] = i\n",
        "                    self._cycle_set.add(i)\n",
        "                    ret = self._hamil_util()\n",
        "                    if(ret):\n",
        "                        return True\n",
        "                    else:\n",
        "                        # remove the element and backtrack\n",
        "                        self._cycle_set.remove(self._cycle[self._index])\n",
        "                        self._index -= 1\n",
        "            # if all neighbors in cycle set\n",
        "            return False\n",
        "\n",
        "    def _get_cycle(self):\n",
        "        \"\"\"\n",
        "        given a square board size, calculate a hamiltonian cycle through\n",
        "        the graph, use it to follow the board, the _cycle variable is a list\n",
        "        of tuples which tells the next coordinates to go to\n",
        "        note that the board starts at row 1, col 1\n",
        "        \"\"\"\n",
        "        self._start_point = 1*self._board_size + 1\n",
        "        self._cycle = np.zeros(((self._board_size-2) ** 2,))\n",
        "        # calculate the cycle path, start at 0, 0\n",
        "        self._index = 0\n",
        "        self._cycle[self._index] = self._start_point\n",
        "        self._cycle_set = set([self._start_point])\n",
        "        cycle_possible = self._hamil_util()\n",
        "\n",
        "    def _get_cycle_square(self):\n",
        "        \"\"\"\n",
        "        simple implementation to get the hamiltonian cycle\n",
        "        for square board, by traversing in a up and down fashion\n",
        "        all movement code is based on this implementation\n",
        "        \"\"\"\n",
        "        self._cycle = np.zeros(((self._board_size-2) ** 2,), dtype=np.int64)\n",
        "        index = 0\n",
        "        sp = 1*self._board_size + 1\n",
        "        while(index < self._cycle.shape[0]):\n",
        "            if(index == 0):\n",
        "                # put as is\n",
        "                pass\n",
        "            elif((sp//self._board_size) == 2 and (sp%self._board_size) == self._board_size-2):\n",
        "                # at the point where we go up and then left to\n",
        "                # complete the cycle, go up once\n",
        "                sp = ((sp//self._board_size)-1)*self._board_size + (sp%self._board_size)\n",
        "            elif(index != 1 and sp//self._board_size == 1):\n",
        "                # keep going left to complete cycle\n",
        "                sp = ((sp//self._board_size))*self._board_size + ((sp%self._board_size)-1)\n",
        "            elif((sp%self._board_size)%2 == 1):\n",
        "                # go down till possible\n",
        "                sp = ((sp//self._board_size)+1)*self._board_size + (sp%self._board_size)\n",
        "                if(sp//self._board_size == self._board_size-1):\n",
        "                    # should have turned right instead of goind down\n",
        "                    sp = ((sp//self._board_size)-1)*self._board_size + ((sp%self._board_size)+1)\n",
        "            else:\n",
        "                # go up till the last but one row\n",
        "                sp = ((sp//self._board_size)-1)*self._board_size + (sp%self._board_size)\n",
        "                if(sp//self._board_size == 1):\n",
        "                    # should have turned right instead of goind up\n",
        "                    sp = ((sp//self._board_size)+1)*self._board_size + ((sp%self._board_size)+1)\n",
        "            self._cycle[index] = sp\n",
        "            index += 1\n",
        "\n",
        "    def move(self, board, legal_moves, values):\n",
        "        \"\"\" get the action using agent policy \"\"\"\n",
        "        cy_len = (self._board_size-2)**2\n",
        "        curr_head = np.sum(self._board_grid * \\\n",
        "            (board[:,:,0]==values['head']).reshape(self._board_size, self._board_size))\n",
        "        index = 0\n",
        "        while(1):\n",
        "            if(self._cycle[index] == curr_head):\n",
        "                break\n",
        "            index = (index+1)%cy_len\n",
        "        prev_head = self._cycle[(index-1)%cy_len]\n",
        "        next_head = self._cycle[(index+1)%cy_len]\n",
        "        # get the next move\n",
        "        if(board[prev_head//self._board_size, prev_head%self._board_size, 0] == 0):\n",
        "            # check if snake is in line with the hamiltonian cycle or not\n",
        "            if(next_head > curr_head):\n",
        "                return 3\n",
        "            else:\n",
        "                return 1\n",
        "        else:\n",
        "            # calcualte intended direction to get move\n",
        "            curr_head_row, curr_head_col = self._point_to_row_col(curr_head)\n",
        "            prev_head_row, prev_head_col = self._point_to_row_col(prev_head)\n",
        "            next_head_row, next_head_col = self._point_to_row_col(next_head)\n",
        "            dx, dy = next_head_col - curr_head_col, -next_head_row + curr_head_row\n",
        "            if(dx == 1 and dy == 0):\n",
        "                return 0\n",
        "            elif(dx == 0 and dy == 1):\n",
        "                return 1\n",
        "            elif(dx == -1 and dy == 0):\n",
        "                return 2\n",
        "            elif(dx == 0 and dy == -1):\n",
        "                return 3\n",
        "            else:\n",
        "                return -1\n",
        "                \n",
        "            \"\"\"\n",
        "            # calculate vectors representing current and new directions\n",
        "            # to get the direction in which to turn\n",
        "            d1 = (curr_head_row - prev_head_row, curr_head_col - prev_head_col)\n",
        "            d2 = (next_head_row - curr_head_row, next_head_col - curr_head_col)\n",
        "            # take cross product\n",
        "            turn_dir = d1[0]*d2[1] - d1[1]*d2[0]\n",
        "            if(turn_dir == 0):\n",
        "                return 1\n",
        "            elif(turn_dir == -1):\n",
        "                return 0\n",
        "            else:\n",
        "                return 2\n",
        "            \"\"\"\n",
        "\n",
        "    def get_action_proba(self, board, values):\n",
        "        \"\"\" for compatibility \"\"\"\n",
        "        move = self.move(board, values)\n",
        "        prob = [0] * self._n_actions\n",
        "        prob[move] = 1\n",
        "        return prob\n",
        "\n",
        "    def _get_model_outputs(self, board=None, model=None):\n",
        "        \"\"\" for compatibility \"\"\" \n",
        "        return [[0] * self._n_actions]\n",
        "\n",
        "    def load_model(self, **kwargs):\n",
        "        \"\"\" for compatibility \"\"\"\n",
        "        pass\n",
        "\n",
        "class SupervisedLearningAgent(DeepQLearningAgent):\n",
        "    \"\"\"This agent learns in a supervised manner. A close to perfect\n",
        "    agent is first used to generate training data, playing only for\n",
        "    a few frames at a time, and then the actions taken by the perfect agent\n",
        "    are used as targets. This helps learning of feature representation\n",
        "    and can speed up training of DQN agent later.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    _model_action_out : TensorFlow Softmax layer\n",
        "        A softmax layer on top of the DQN model to train as a classification\n",
        "        problem (instead of regression)\n",
        "    _model_action : TensorFlow Model\n",
        "        The model that will be trained and is simply DQN model + softmax\n",
        "    \"\"\"\n",
        "    def __init__(self, board_size=10, frames=2, buffer_size=10000,\n",
        "                 gamma=0.99, n_actions=3, use_target_net=True,\n",
        "                 version=''):\n",
        "        \"\"\"Initializer for SupervisedLearningAgent, similar to DeepQLearningAgent\n",
        "        but creates extra layer and model for classification training\n",
        "        \"\"\"        \n",
        "        DeepQLearningAgent.__init__(self, board_size=board_size, frames=frames, buffer_size=buffer_size,\n",
        "                 gamma=gamma, n_actions=n_actions, use_target_net=use_target_net,\n",
        "                 version=version)\n",
        "        # define model with softmax activation, and use action as target\n",
        "        # instead of the reward value\n",
        "        self._model_action_out = Softmax()(self._model.get_layer('action_values').output)\n",
        "        self._model_action = Model(inputs=self._model.get_layer('input').input, outputs=self._model_action_out)\n",
        "        self._model_action.compile(optimizer=Adam(0.0005), loss='categorical_crossentropy')\n",
        "        \n",
        "    def train_agent(self, batch_size=32, num_games=1, epochs=5, \n",
        "                    reward_clip=False):\n",
        "        \"\"\"Train the model by sampling from buffer and return the error.\n",
        "        _model_action is trained as a classification problem to learn weights\n",
        "        for all the layers of the DQN model\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size : int, optional\n",
        "            The number of examples to sample from buffer, should be small\n",
        "        num_games : int, optional\n",
        "            Not used here, kept for consistency with other agents\n",
        "        epochs : int, optional\n",
        "            Number of epochs to train the model for\n",
        "        reward_clip : bool, optional\n",
        "            Not used here, kept for consistency with other agents\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            loss : float\n",
        "            The current error (error metric is cross entropy)\n",
        "        \"\"\"\n",
        "        s, a, _, _, _, _ = self._buffer.sample(self.get_buffer_size())\n",
        "        # fit using the actions as assumed to be best\n",
        "        history = self._model_action.fit(self._normalize_board(s), a, epochs=epochs)\n",
        "        loss = round(history.history['loss'][-1], 5)\n",
        "        # loss = self._model_action.evaluate(self._normalize_board(s), a, verbose=0)\n",
        "        return loss\n",
        "\n",
        "    def get_max_output(self):\n",
        "        \"\"\"Get the maximum output of Q values from the model\n",
        "        This value is used to later divide the weights of the output layer\n",
        "        of DQN model since the values can be unexpectedly high because\n",
        "        we are training the classification model (which disregards the relative\n",
        "        magnitudes of the linear outputs)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        max_value : int\n",
        "            The maximum output produced by the network (_model)\n",
        "        \"\"\"\n",
        "        s, _, _, _, _, _ = self._buffer.sample(self.get_buffer_size())\n",
        "        max_value = np.max(np.abs(self._model.predict(self._normalize_board(s))))\n",
        "        return max_value\n",
        "\n",
        "    def normalize_layers(self, max_value=None):\n",
        "        \"\"\"Use the max value to divide the weights of the last layer\n",
        "        of the DQN model, this helps stabilize the initial training of DQN\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        max_value : int, optional\n",
        "            Value by which to divide, assumed to be 1 if None\n",
        "        \"\"\"\n",
        "        # normalize output layers by this value\n",
        "        if(max_value is None or np.isnan(max_value)):\n",
        "            max_value = 1.0\n",
        "        # dont normalize all layers as that will shrink the\n",
        "        # output proportional to the no of layers\n",
        "        self._model.get_layer('action_values').set_weights(\\\n",
        "           [x/max_value for x in self._model.get_layer('action_values').get_weights()])\n",
        "\n",
        "class BreadthFirstSearchAgent(Agent):\n",
        "    \"\"\"\n",
        "    finds the shortest path from head to food\n",
        "    while avoiding the borders and body\n",
        "    \"\"\"\n",
        "    def _get_neighbors(self, point, values, board):\n",
        "        \"\"\"\n",
        "        point is a single integer such that \n",
        "        row = point//self._board_size\n",
        "        col = point%self._board_size\n",
        "        \"\"\"\n",
        "        row, col = self._point_to_row_col(point)\n",
        "        neighbors = []\n",
        "        for delta_row, delta_col in [[-1,0], [1,0], [0,1], [0,-1]]:\n",
        "            new_row, new_col = row + delta_row, col + delta_col\n",
        "            if(board[new_row][new_col] in \\\n",
        "               [values['board'], values['food'], values['head']]):\n",
        "                neighbors.append(new_row*self._board_size + new_col)\n",
        "        return neighbors\n",
        "\n",
        "    def _get_shortest_path(self, board, values):\n",
        "        # get the head coordinate\n",
        "        board = board[:,:,0]\n",
        "        head = ((self._board_grid * (board == values['head'])).sum())\n",
        "        points_to_search = deque()\n",
        "        points_to_search.append(head)\n",
        "        path = []\n",
        "        row, col = self._point_to_row_col(head)\n",
        "        distances = np.ones((self._board_size, self._board_size)) * np.inf\n",
        "        distances[row][col] = 0\n",
        "        visited = np.zeros((self._board_size, self._board_size))\n",
        "        visited[row][col] = 1\n",
        "        found = False\n",
        "        while(not found):\n",
        "            if(len(points_to_search) == 0):\n",
        "                # complete board has been explored without finding path\n",
        "                # take any arbitrary action\n",
        "                path = []\n",
        "                break\n",
        "            else:\n",
        "                curr_point = points_to_search.popleft()\n",
        "                curr_row, curr_col = self._point_to_row_col(curr_point)\n",
        "                n = self._get_neighbors(curr_point, values, board)\n",
        "                if(len(n) == 0):\n",
        "                    # no neighbors available, explore other paths\n",
        "                    continue\n",
        "                # iterate over neighbors and calculate distances\n",
        "                for p in n:\n",
        "                    row, col = self._point_to_row_col(p)\n",
        "                    if(distances[row][col] > 1 + distances[curr_row][curr_col]):\n",
        "                        # update shortest distance\n",
        "                        distances[row][col] = 1 + distances[curr_row][curr_col]\n",
        "                    if(board[row][col] == values['food']):\n",
        "                        # reached food, break\n",
        "                        found = True\n",
        "                        break\n",
        "                    if(visited[row][col] == 0):\n",
        "                        visited[curr_row][curr_col] = 1\n",
        "                        points_to_search.append(p)\n",
        "        # create the path going backwards from the food\n",
        "        curr_point = ((self._board_grid * (board == values['food'])).sum())\n",
        "        path.append(curr_point)\n",
        "        while(1):\n",
        "            curr_row, curr_col = self._point_to_row_col(curr_point)\n",
        "            if(distances[curr_row][curr_col] == np.inf):\n",
        "                # path is not possible\n",
        "                return []\n",
        "            if(distances[curr_row][curr_col] == 0):\n",
        "                # path is complete\n",
        "                break\n",
        "            n = self._get_neighbors(curr_point, values, board)\n",
        "            for p in n:\n",
        "                row, col = self._point_to_row_col(p)\n",
        "                if(distances[row][col] != np.inf and \\\n",
        "                   distances[row][col] == distances[curr_row][curr_col] - 1):\n",
        "                    path.append(p)\n",
        "                    curr_point = p\n",
        "                    break\n",
        "        return path\n",
        "\n",
        "    def move(self, board, legal_moves, values):\n",
        "        if(board.ndim == 3):\n",
        "            board = board.reshape((1,) + board.shape)\n",
        "        board_main = board.copy()\n",
        "        a = np.zeros((board.shape[0],), dtype=np.uint8)\n",
        "        for i in range(board.shape[0]):\n",
        "            board = board_main[i,:,:,:]\n",
        "            path = self._get_shortest_path(board, values)\n",
        "            if(len(path) == 0):\n",
        "                a[i] = 1\n",
        "                continue\n",
        "            next_head = path[-2]\n",
        "            curr_head = (self._board_grid * (board[:,:,0] == values['head'])).sum()\n",
        "            # get prev head position\n",
        "            if(((board[:,:,0] == values['head']) + (board[:,:,0] == values['snake']) \\\n",
        "                == (board[:,:,1] == values['head']) + (board[:,:,1] == values['snake'])).all()):\n",
        "                # we are at the first frame, snake position is unchanged\n",
        "                prev_head = curr_head - 1\n",
        "            else:\n",
        "                # we are moving\n",
        "                prev_head = (self._board_grid * (board[:,:,1] == values['head'])).sum()\n",
        "            curr_head_row, curr_head_col = self._point_to_row_col(curr_head)\n",
        "            prev_head_row, prev_head_col = self._point_to_row_col(prev_head)\n",
        "            next_head_row, next_head_col = self._point_to_row_col(next_head)\n",
        "            dx, dy = next_head_col - curr_head_col, -next_head_row + curr_head_row\n",
        "            if(dx == 1 and dy == 0):\n",
        "                a[i] = 0\n",
        "            elif(dx == 0 and dy == 1):\n",
        "                a[i] = 1\n",
        "            elif(dx == -1 and dy == 0):\n",
        "                a[i] = 2\n",
        "            elif(dx == 0 and dy == -1):\n",
        "                a[i] = 3\n",
        "            else:\n",
        "                a[i] = 0\n",
        "        return a\n",
        "        \"\"\"\n",
        "        d1 = (curr_head_row - prev_head_row, curr_head_col - prev_head_col)\n",
        "        d2 = (next_head_row - curr_head_row, next_head_col - curr_head_col)\n",
        "        # take cross product\n",
        "        turn_dir = d1[0]*d2[1] - d1[1]*d2[0]\n",
        "        if(turn_dir == 0):\n",
        "            return 1\n",
        "        elif(turn_dir == -1):\n",
        "            return 0\n",
        "        else:\n",
        "            return 2\n",
        "        \"\"\"\n",
        "\n",
        "    def get_action_proba(self, board, values):\n",
        "        \"\"\" for compatibility \"\"\"\n",
        "        move = self.move(board, values)\n",
        "        prob = [0] * self._n_actions\n",
        "        prob[move] = 1\n",
        "        return prob\n",
        "\n",
        "    def _get_model_outputs(self, board=None, model=None):\n",
        "        \"\"\" for compatibility \"\"\" \n",
        "        return [[0] * self._n_actions]\n",
        "\n",
        "    def load_model(self, **kwargs):\n",
        "        \"\"\" for compatibility \"\"\"\n",
        "        pass\n",
        "\n"
      ],
      "metadata": {
        "id": "no3EsjeIV48U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NMT9mI6pVP3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Script for training the agent for snake using various methods\n",
        "'''\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import deque\n",
        "import pandas as pd\n",
        "import time\n",
        "from utils import play_game, play_game2\n",
        "from game_environment import Snake, SnakeNumpy\n",
        "import tensorflow as tf\n",
        "from agent import DeepQLearningAgent, PolicyGradientAgent,\\\n",
        "                AdvantageActorCriticAgent, mean_huber_loss\n",
        "import json\n",
        "\n",
        "# some global variables\n",
        "tf.random.set_seed(42)\n",
        "version = 'v17.1'\n",
        "\n",
        "# get training configurations\n",
        "with open('model_config/{:s}.json'.format(version), 'r') as f:\n",
        "    m = json.loads(f.read())\n",
        "    board_size = m['board_size']\n",
        "    frames = m['frames'] # keep frames >= 2\n",
        "    max_time_limit = m['max_time_limit']\n",
        "    supervised = bool(m['supervised'])\n",
        "    n_actions = m['n_actions']\n",
        "    obstacles = bool(m['obstacles'])\n",
        "    buffer_size = m['buffer_size']\n",
        "\n",
        "# define no of episodes, logging frequency\n",
        "#episodes = 2 * (10**5)\n",
        "episodes = 6250\n",
        "log_frequency = 500\n",
        "games_eval = 8\n",
        "\n",
        "# setup the agent\n",
        "agent = DeepQLearningAgent(board_size=board_size, frames=frames, n_actions=n_actions, \n",
        "                           buffer_size=buffer_size, version=version)\n",
        "# agent = PolicyGradientAgent(board_size=board_size, frames=frames, n_actions=n_actions, \n",
        "        # buffer_size=2000, version=version)\n",
        "# agent = AdvantageActorCriticAgent(board_size=board_size, frames=frames, n_actions=n_actions, \n",
        "                                  # buffer_size=10000, version=version)\n",
        "# agent.print_models()\n",
        "\n",
        "# check in the same order as class hierarchy\n",
        "if(isinstance(agent, DeepQLearningAgent)):\n",
        "    agent_type = 'DeepQLearningAgent'\n",
        "if(isinstance(agent, PolicyGradientAgent)):\n",
        "    agent_type = 'PolicyGradientAgent'\n",
        "if(isinstance(agent, AdvantageActorCriticAgent)):\n",
        "    agent_type = 'AdvantageActorCriticAgent'\n",
        "print('Agent is {:s}'.format(agent_type))\n",
        "\n",
        "# setup the epsilon range and decay rate for epsilon\n",
        "# define rewrad type and update frequency, see utils for more details\n",
        "if(agent_type in ['DeepQLearningAgent']):\n",
        "    epsilon, epsilon_end = 1, 0.01\n",
        "    reward_type = 'current'\n",
        "    sample_actions = False\n",
        "    n_games_training = 8*16\n",
        "    decay = 0.97\n",
        "    if(supervised):\n",
        "        # lower the epsilon since some starting policy has already been trained\n",
        "        epsilon = 0.01\n",
        "        # load the existing model from a supervised method\n",
        "        # or some other pretrained model\n",
        "        agent.load_model(file_path='models/{:s}'.format(version))\n",
        "        # agent.set_weights_trainable()\n",
        "if(agent_type in ['PolicyGradientAgent']):\n",
        "    epsilon, epsilon_end = -1, -1\n",
        "    reward_type = 'discounted_future'\n",
        "    sample_actions = True\n",
        "    exploration_threshold = 0.1\n",
        "    n_games_training = 16\n",
        "    decay = 1\n",
        "if(agent_type in ['AdvantageActorCriticAgent']):\n",
        "    epsilon, epsilon_end = -1, -1\n",
        "    reward_type = 'current'\n",
        "    sample_actions = True\n",
        "    exploration_threshold = 0.1\n",
        "    n_games_training = 32\n",
        "    decay = 1\n",
        "\n",
        "# decay = np.exp(np.log((epsilon_end/epsilon))/episodes)\n",
        "\n",
        "# use only for DeepQLearningAgent\n",
        "if(agent_type in ['DeepQLearningAgent']):\n",
        "    # play some games initially to fill the buffer\n",
        "    # or load from an existing buffer (supervised)\n",
        "    if(supervised):\n",
        "        try:\n",
        "            agent.load_buffer(file_path='models/{:s}'.format(version), iteration=1)\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "    else:\n",
        "        # setup the environment\n",
        "        games = 512\n",
        "        env = SnakeNumpy(board_size=board_size, frames=frames, \n",
        "                    max_time_limit=max_time_limit, games=games,\n",
        "                    frame_mode=True, obstacles=obstacles, version=version)\n",
        "        ct = time.time()\n",
        "        _ = play_game2(env, agent, n_actions, n_games=games, record=True,\n",
        "                       epsilon=epsilon, verbose=True, reset_seed=False,\n",
        "                       frame_mode=True, total_frames=games*64)\n",
        "        print('Playing {:d} frames took {:.2f}s'.format(games*64, time.time()-ct))\n",
        "\n",
        "env = SnakeNumpy(board_size=board_size, frames=frames, \n",
        "            max_time_limit=max_time_limit, games=n_games_training,\n",
        "            frame_mode=True, obstacles=obstacles, version=version)\n",
        "env2 = SnakeNumpy(board_size=board_size, frames=frames, \n",
        "            max_time_limit=max_time_limit, games=games_eval,\n",
        "            frame_mode=True, obstacles=obstacles, version=version)\n",
        "\n",
        "# training loop\n",
        "model_logs = {'iteration':[], 'reward_mean':[],\n",
        "              'length_mean':[], 'games':[], 'loss':[]}\n",
        "for index in tqdm(range(episodes)):\n",
        "    if(agent_type in ['DeepQLearningAgent']):\n",
        "        # make small changes to the buffer and slowly train\n",
        "        _, _, _ = play_game2(env, agent, n_actions, epsilon=epsilon,\n",
        "                       n_games=n_games_training, record=True,\n",
        "                       sample_actions=sample_actions, reward_type=reward_type,\n",
        "                       frame_mode=True, total_frames=n_games_training, \n",
        "                       stateful=True)\n",
        "        loss = agent.train_agent(batch_size= 64,\n",
        "                                 num_games=n_games_training, reward_clip=True)\n",
        "\n",
        "    if(agent_type in ['AdvantageActorCriticAgent']):\n",
        "        # play a couple of games and train on all\n",
        "        _, _, total_games = play_game2(env, agent, n_actions, epsilon=epsilon,\n",
        "                       n_games=n_games_training, record=True,\n",
        "                       sample_actions=sample_actions, reward_type=reward_type,\n",
        "                       frame_mode=True, total_games=n_games_training*2)\n",
        "        loss = agent.train_agent(batch_size=agent.get_buffer_size(), \n",
        "                                 num_games=total_games, reward_clip=True)\n",
        "\n",
        "    if(agent_type in ['PolicyGradientAgent', 'AdvantageActorCriticAgent']):\n",
        "        # for policy gradient algorithm, we only take current episodes for training\n",
        "        agent.reset_buffer()\n",
        "\n",
        "    # check performance every once in a while\n",
        "    if((index+1)%log_frequency == 0):\n",
        "        # keep track of agent rewards_history\n",
        "        current_rewards, current_lengths, current_games = \\\n",
        "                    play_game2(env2, agent, n_actions, n_games=games_eval, epsilon=-1,\n",
        "                               record=False, sample_actions=False, frame_mode=True, \n",
        "                               total_frames=-1, total_games=games_eval)\n",
        "        \n",
        "        model_logs['iteration'].append(index+1)\n",
        "        model_logs['reward_mean'].append(round(int(current_rewards)/current_games, 2))\n",
        "        # model_logs['reward_dev'].append(round(np.std(current_rewards), 2))\n",
        "        model_logs['length_mean'].append(round(int(current_lengths)/current_games, 2))\n",
        "        model_logs['games'].append(current_games)\n",
        "        model_logs['loss'].append(loss)\n",
        "        pd.DataFrame(model_logs)[['iteration', 'reward_mean', 'length_mean', 'games', 'loss']]\\\n",
        "          .to_csv('model_logs/{:s}.csv'.format(version), index=False)\n",
        "\n",
        "    # copy weights to target network and save models\n",
        "    if((index+1)%log_frequency == 0):\n",
        "        agent.update_target_net()\n",
        "        agent.save_model(file_path='models/{:s}'.format(version), iteration=(index+1))\n",
        "        # keep some epsilon alive for training\n",
        "        epsilon = max(epsilon * decay, epsilon_end)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy2_3K6GUmng",
        "outputId": "e6841588-9dcf-4e6d-ad16-7c8a4db184b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent is DeepQLearningAgent\n",
            "Playing 32768 frames took 1.26s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6250/6250 [02:57<00:00, 35.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Script  to visualize how agent plays a game, useful to study different iterations\n",
        "'''\n",
        "import numpy as np\n",
        "from agent import DeepQLearningAgent, PolicyGradientAgent, \\\n",
        "        AdvantageActorCriticAgent, HamiltonianCycleAgent, BreadthFirstSearchAgent\n",
        "from game_environment import Snake, SnakeNumpy\n",
        "from utils import visualize_game\n",
        "import json\n",
        "# import keras.backend as K\n",
        "\n",
        "# some global variables\n",
        "version = 'v17.1'\n",
        "\n",
        "with open('model_config/{:s}.json'.format(version), 'r') as f:\n",
        "    m = json.loads(f.read())\n",
        "    board_size = m['board_size']\n",
        "    frames = m['frames'] # keep frames >= 2\n",
        "    max_time_limit = m['max_time_limit']\n",
        "    supervised = bool(m['supervised'])\n",
        "    n_actions = m['n_actions']\n",
        "    obstacles = bool(m['obstacles'])\n",
        "\n",
        "iteration_list = [163500]\n",
        "max_time_limit = 398\n",
        "\n",
        "# setup the environment\n",
        "env = Snake(board_size=board_size, frames=frames, max_time_limit=max_time_limit,\n",
        "            obstacles=obstacles, version=version)\n",
        "s = env.reset()\n",
        "n_actions = env.get_num_actions()\n",
        "\n",
        "# setup the agent\n",
        "# K.clear_session()\n",
        "agent = DeepQLearningAgent(board_size=board_size, frames=frames, \n",
        "                           n_actions=n_actions, buffer_size=10, version=version)\n",
        "# agent = PolicyGradientAgent(board_size=board_size, frames=frames, n_actions=n_actions, buffer_size=10)\n",
        "# agent = AdvantageActorCriticAgent(board_size=board_size, frames=frames, n_actions=n_actions, buffer_size=10)\n",
        "# agent = HamiltonianCycleAgent(board_size=board_size, frames=frames, n_actions=n_actions, buffer_size=10)\n",
        "# agent = BreadthFirstSearchAgent(board_size=board_size, frames=frames, n_actions=n_actions, buffer_size=10)\n",
        "\n",
        "for iteration in iteration_list:\n",
        "    agent.load_model(file_path='models/{:s}'.format(version), iteration=iteration)\n",
        "    \n",
        "    for i in range(5):\n",
        "        visualize_game(env, agent,\n",
        "            path='images/game_visual_{:s}_{:d}_14_ob_{:d}.mp4'.format(version, iteration, i),\n",
        "            debug=False, animate=True, fps=12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6WeVRl1CWDlm",
        "outputId": "aacdf01f-fcbe-4033-db02-87fddd39c58a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Visualization\n",
            "Game ran for 128 frames\n",
            "Starting Visualization\n",
            "Game ran for 400 frames\n",
            "Starting Visualization\n",
            "Game ran for 400 frames\n",
            "Starting Visualization\n",
            "Game ran for 400 frames\n",
            "Starting Visualization\n",
            "Game ran for 17 frames\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}